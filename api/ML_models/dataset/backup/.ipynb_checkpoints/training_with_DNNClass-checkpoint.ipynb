{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "df = pd.read_csv('samples_nylonGuitar_1024_Mm7_R03.csv')\n",
    "\n",
    "X = np.array(df.iloc[:,:-1], dtype=np.float)\n",
    "y = np.array(df.iloc[:,-1], dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "processedData_path = \"preprocessedSamples.data\"\n",
    "\n",
    "if os.path.isfile(processedData_path): #if already preprocessed\n",
    "    df_new = pd.read_pickle(processedData_path)\n",
    "else:\n",
    "    for i in range(len(X)):\n",
    "        sample = np.array(X[i], dtype=np.float)\n",
    "        sample = sample*np.hamming(1024)\n",
    "        sample = np.abs(np.fft.rfft(sample))[1:]\n",
    "        sample = np.append(sample, y[i])\n",
    "        try:\n",
    "            df_new = np.vstack([df_new, sample])\n",
    "        except:\n",
    "            df_new = np.array(sample, dtype=np.float)\n",
    "        if i % 200 == 0:\n",
    "            print(i)\n",
    "            df_new[i]\n",
    "    \n",
    "    df_new = pd.DataFrame(df_new)\n",
    "    \n",
    "    df_new.to_pickle(processedData_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "8.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD/CAYAAADllv3BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XuUXGWd7vHvr6r6kvQlt+5ckElCgCQQEZEgjOLAjIiC\nesYl4yy5zFqOR5gzDq6zRjyjg6IRQTwH1/KI46AoAnITUHBEPYqg4RKuHbkm5ELu93Qn6Xt3dV3e\n88feVb13d1e6012d7r3zfNbq1VV776p630rqqbff993vNuccIiISH4mJLoCIiJSXgl1EJGYU7CIi\nMaNgFxGJGQW7iEjMKNhFRGJGwS4iEjMKdhGRmFGwi4jETGoiXrShocEtXLhwIl5aRCSyVq9e3eKc\naxzuuAkJ9oULF9LU1DQRLy0iEllmtm0kx6krRkQkZhTsIiIxo2AXEYkZBbuISMwo2EVEYkbBLiIS\nMwp2EZGYiWywZ3N5HnhpO5lcfqKLIiIyqUzICUrl8MOnNnPz79eTTCT4uzOPn+jiiIhMGpFtsT+5\noRmA+urIfjeJiIyLyAb7qztaAahMRbYKIiLjIrKpmM56fetugsshIjLZRDbYi5TsIiIhkQ92p2QX\nEQmJfLDnNdtRRCRk2GA3syozu93MtplZh5m9bGYX+fsWmpkzs87Az3XjX+x+aq+LiISNZK5gCtgB\nnAdsBy4GHjSz0wLHTHfOZcehfMNyTtEuIhI0bIvdOdflnFvhnNvqnMs7534NbAHOHP/iDU+xLiIS\ndsR97GY2B1gMrAls3mZmO83sDjNrKFvpRkANdhGRsCMKdjOrAO4F7nLOrQNagLOABXgt+Dp//1CP\nvcrMmsysqbm5eWylDlBXjIhI2IiD3cwSwN1AH3A1gHOu0znX5JzLOuf2+dsvNLP6gY93zt3mnFvu\nnFve2DjsRbZHTLEuIhI2ooVWzMyA24E5wMXOuUyJQws5a2Uo24iowS4iEjbSFbRuBU4BLnDO9RQ2\nmtnZQCuwEZgB3AKsdM61lbugpegEJRGRsJHMY18A/BPwTmBvYL765cAi4HdAB/AGkAYuHcfyDpJX\nrouIhAzbYnfObePwXSv3l684R06DpyIiYZFfUkBERMIiH+xqsIuIhEU/2DV4KiISEvlg1+qOIiJh\nkQ92tddFRMKiH+zqZBcRCYl+sE90AUREJpnoB7ta7CIiITEI9okugYjI5BL9YJ/oAoiITDLRD3Yl\nu4hISPSDXW12EZGQyAe7VncUEQmLfLCrL0ZEJCzywa5YFxEJi36wK9lFREIiH+x5JbuISEjkg125\nLiISFv1gn+gCiIhMMtEPdjXZRURCIh/sIiISFvlg1+CpiEhY5INduS4iEhb9YJ/oAoiITDLRD3Yl\nu4hISPSDXW12EZGQ6Ae7cl1EJGTYYDezKjO73cy2mVmHmb1sZhcF9r/fzNaZWbeZ/cnMFoxvkcM0\nj11EJGwkLfYUsAM4D5gGXAc8aGYLzawBeNjfNhNoAh4Yp7IOSbkuIhKWGu4A51wXsCKw6ddmtgU4\nE5gFrHHOPQRgZiuAFjNb6pxbV/7iDlG+o/EiIiIRcsR97GY2B1gMrAGWAa8W9vlfApv87UeFTlAS\nEQk7omA3swrgXuAuv0VeC7QNOKwNqBvisVeZWZOZNTU3N4+2vIMo10VEwkYc7GaWAO4G+oCr/c2d\nQP2AQ+uBjoGPd87d5pxb7pxb3tjYOMriDqZcFxEJG1Gwm5kBtwNzgEuccxl/1xrg9MBxNcCJ/vZx\nE5oJoya7iEjISFvstwKnAB91zvUEtj8CvN3MLjGzauCrwGvjPXAayvXxfCERkQgayTz2BcA/Ae8E\n9ppZp/9zuXOuGbgEuBE4BJwNfHI8CwzhMNfgqYhI2EimO24D7DD7HweWlrNQR0K5LiISFsklBYJ9\n7Mp1EZGwaAZ78LaSXUQkJJrBHpoUo2QXEQmKZrCjrhgRkVKiGexqsYuIlBTJYA9SrouIhEUy2HWC\nkohIadEM9kCc6wQlEZGwaAa7looRESkpmsE+0QUQEZnEohnswTNP1WQXEQmJZrAHbueV6yIiIdEM\n9tCsGCW7iEhQJIMdDZ6KiJQUyWDXkgIiIqVFM9jVYhcRKSmSwR6kWTEiImGRDHatxy4iUlo0gz10\nBSUlu4hIUDSDPXhbuS4iEhLNYA+EuU5QEhEJi2awo64YEZFSIhns4b6YCSuFiMikFMlgV66LiJQW\nzWDXNU9FREqKZrCHrqA0gQUREZmEohnsuuapiEhJ0Qz24G11xYiIhIwo2M3sajNrMrO0md0Z2L7Q\nzJyZdQZ+rhu30vrCZ56KiEhQaoTH7QZuAD4ITBli/3TnXLZspRqGBk9FREobUbA75x4GMLPlwPHj\nWqIjpFwXEQkrVx/7NjPbaWZ3mFlDmZ6zJK3HLiJS2liDvQU4C1gAnAnUAfcOdaCZXeX30zc1NzeP\n6UW1pICISGljCnbnXKdzrsk5l3XO7QOuBi40s/ohjr3NObfcObe8sbFxLC874HnL9lQiIrFQ7umO\nhZi1Mj9v+EW0uqOISEkjGjw1s5R/bBJImlk1kMXrfmkFNgIzgFuAlc65tvEprscd5p6IyLFupC32\nrwA9wJeAK/zbXwEWAb8DOoA3gDRwafmLGRaax65cFxEJGel0xxXAihK77y9XYUZKqzuKiJQWzSUF\nQn3sinYRkaBIBnuwna5cFxEJi2Swa3VHEZHSohnswdtqsouIhEQz2JXlIiIlRTPYQ1dQUsqLiARF\nM9i1CJiISEkKdhGRmIlmsGt1RxGRkqIZ7FoETESkpEgGe8gogv2xNXtZ+KXfsKu1p/zlERGZYJEP\n9tF0xTzYtAOANbvGdRFKEZEJEclgH+vgaaH7JpkY12XjRUQmRDSDPTR4euRyfrInTMEuIvETzWAf\n4+qOhcck1GIXkRiKZrAHb4+iyV5osSfVYheRGIpmsLuxdcX0t9jLVCARkUkkktFWvGK2Maomez7v\n/VYfu4jEUTSDvTCrxWxUJyjl/CfQrBgRiaNIBnuhzZ4wG9U89pxOVxWRGItksBda7GajGzwt9NEr\n4EUkjqIZ7P7vhNnoZsX4D9Ja7iISR9EMdj+PEzbaE5TCzyMiEicRDfZAH/uoZsWoK0ZE4iuawe7/\nHm0fe15dMSISY9EM9kJXTGKUs2L8J1Cui0gcRTPYg9MdR9Niz6vFLiLxNaJgN7OrzazJzNJmdueA\nfe83s3Vm1m1mfzKzBeNS0qDi4KmNKpxzmu4oIjE20hb7buAG4CfBjWbWADwMXAfMBJqAB8pZwMMZ\n7ayYwpICynURiaPUSA5yzj0MYGbLgeMDuz4OrHHOPeTvXwG0mNlS59y6Mpe1vzz+74SNLtnzxT52\nJbuIxM9Y+9iXAa8W7jjnuoBN/vZxM9Z57P2zYspXJhGRyWKswV4LDLxwaBtQN/BAM7vK76dvam5u\nHtOLFgZPbbTz2P2H5NRiF5EYGmuwdwL1A7bVAx0DD3TO3eacW+6cW97Y2DimFy2u7pgY3eqOhVkx\n6ooRkTgaa7CvAU4v3DGzGuBEf/u46e9jZ0zz2DXdUUTiaKTTHVNmVg0kgaSZVZtZCngEeLuZXeLv\n/yrw2ngOnEJgSYGEFWe4HInCNMfRPFZEZLIbaYv9K0AP8CXgCv/2V5xzzcAlwI3AIeBs4JPjUM6Q\nQjs7Oco+dqc+dhGJsZFOd1wBrCix73FgafmKNAJj7GPPqY9dRGIs0ksKJBM2qlZ3TtMdRSTGohns\ngRb7WJbt1eCpiMRRpIM9McqLWRdPUFKTXURiKJrB7v/2+thHf4KScl1E4iiawe6HedJsTCs0qitG\nROIomsHu/04kxnaxDLXYRSSOohnsoemOY2ixK9lFJIYiGewErqA0pmBXV4yIxFBEg92TGuWSAgVq\nsItIHEUy2MvWFaMWu4jEUDSD3f89mq6Y4AlN6mMXkTiKZrD7eZxKeicoHcnZp8EsV66LSBxFM9gD\ng6dwZFMegy18dcWISBxFM9gDfexwZAEdPKFJqzuKSBxFM9j930krBLt3f19777Bnoga/BLQeu4jE\nUTSDvbCkQKDF/tiavZz9zSf4+eodh32s+thFJO4iGewFwWB/sMkL9F2Heg77mGCLXn3sIhJHkQz2\n4rK9iWBXjHc7mTh8lfKhPvbxKJ2IyMSKZrAXrqBkgwdP+3K5wz421MeuvhgRiaFoBvvAWTF5R85f\nW6Ave/g1BnKa7igiMRePYHeQ87cNF+zBtWWU6yISR9EMdv93cPC02GLPjXy6o1rsIhJH0Qx2Fz7z\nNO8cWT/Qh+2KyauPXUTiLZrB7v9O+qXP5/tb3325wwe70zx2EYm5SAZ7QXBWTNZP6Udf3c1zmw6U\nfExw8FRLCohIHEUz2AfNY3ehbpVLf/R8yYfqBCURibtIBvugeez5kfeXu9A89vKXTURkokUz2AvT\nHZNDt9gPR10xIhJ3ZQl2M1tpZr1m1un/rC/H85YyeHXH/j724agrRkTirpwt9qudc7X+z5IyPu8g\nQ52gNNLL3GlWjIjEXTS7Ygp97Imxtdi1HruIxFE5g/0mM2sxs1Vmdv7AnWZ2lZk1mVlTc3PzmF5o\nqCsojbSPPa8+dhGJuXIF+xeBRcDbgNuAR83sxOABzrnbnHPLnXPLGxsbx/RihThOjGJWTGhJAc2K\nEZEYKkuwO+decM51OOfSzrm7gFXAxeV47hIvCIy2K6b/tgZPRSSOxquP3VG48sU4PTmEg31gSJca\nTNUiYCISd2MOdjObbmYfNLNqM0uZ2eXAXwG/H3vxhlbsYw9czDo74GyjdInFwPKh6Y7jUz4RkYmU\nKsNzVAA3AEuBHLAO+JhzbtzmshdWcKyuSAJDD572ZHJMqUwOeqwutCEicTfmYHfONQNnlaEsI9ab\n8S5/N9UPbufcoKmLPZmhL5EXzH8t2ysicRTJeey92RyphFHhr9ub82fFXHjqHBY11ADQ01ci2PP9\nA69qsItIHEUz2DN5qiuSJArrsfuzYpbMreNLFy31jxk62Aut9FTC1BUjIrEU0WDPUV2RKM5jz+Ud\nznmt8EK/e6lgL4R5RTKhYBeRWIposOepSiWLwZ7xZ8QkzahMeVUqdSWlQpinkqYTlEQklqIZ7Nkc\nVRUJ/GnsfOqOlwBvGd9Cv3smcFHrdXvbue2pTUD/4Gl1KjnsZfRERKKoHNMdj7p0Jkd1Klm8glJB\nKmFUFoI9MI/9ou8+jXPw6feeUOxjr65IFFv6IiJxEs0WeyYf6mMvSJhRkfK2feanTfxx3T6g/4Sm\nnkyu2BVTXZEszocXEYmTSAZ7OpvzZsUMWLQgOAUS4PpH14b2d/f1B3tVRXJELfbeTI723szYCy0i\ncpREMti9wdPBLfZkMlHsiikcF1yat7svV1wErDqVCPXDl/Lx/3yWd6x4rDwFFxE5CiIa7IUW+4Bg\nt3CLPZ3N0drd39ruSmeLJyhVVyRLricTtHZPe5lKLSJydEQz2AtdMQNK73XF9Id9OpvnYHdf8X6w\nK0aDpyISV9EM9lKDpwmjIhVssefp6M0W73f3ZYtryoxk8HTgFZYOdKbZ0tI11uKLiIyrSE537M3k\nQicoFQSnO4J3RmpHYODTa7F7t6tTww+eHujqb+1nc3nOvOFxALZ+68NjrYKIyLiJZIs9ncmHTlAq\nSAyYFQMMaLHnAn3sCbJ5V/KCHAB723qLt9fv6yje1rVSRWQyi1yw5/OOvlzeO0FpQIs9m8sXr6pU\nEG6xZwMnKHlryhzu7NPOdP+Xwlv7O4u3u0qsHCkiMhlELti7+rywrakaHOxDda0MarEH5rGXekxB\ncE33g4FumZaO9ChKLiJydEQu2AtBXVddMWhWTN8Q89IPBWfFpLOhWTHgXY2prXvoE5DSJYL9QJeC\nXUQmrwgHe2pQi32oWS772tPUVaWYWpkccIKS12L/w9p9nHXj4zQP0QrvzfQ/XzDYmzu822t3t3PT\nb98M9bkf6uob8rlERI6WCAa717quq64YUVfM3rZe6qpTzK6r4rVdbaG1YgDe2N1GXy7PvvZecnkX\nCuWSXTGd3jFX/rSJHz61mb3t/YOs7/nWHznrxsfHWk0RkVGLYLAHW+zhfUNdw3RPWw911RVc+u75\nvLjlIDf/3rvGdqErZsfBHsAbKL3wO09y1o2PF4M7eLGOA0MEe2HgdWtLNwAr1+8vfhkcbraNiMh4\nilywFxbkqq9OhZbtveKc+XzqPQsHHb+vPU39lBSfPvcEGmqritsLLfadh7xQ3t+RZlOzd/LRP97x\nEgc604O6Ymoqk0yfWsGBzj7/Oby3b+sB73GFdeEBdrX2jLmuIiKjEblgDw2eBrpibvjYadRUDT7f\nqjOdpa66gopkgr9Z2ljcXjiRaechL4C3Bs4ofX1XG999YmOoK+ZQVx9Tq1I01FbR0pmmL5svXjB7\na0vXoEvxbdzfP+997e52DgVa/G3dGf7t56+yp03hLyLlF+FgH9wVU0pdtRf49dUVgHdt1MIl9AoL\ngT21oTn0mJ8+t427n9tavH+gq4/aqhSzaipp6UzzxV+8Rovfct95qGfQgOnzmw/Slc6yZncbF9/y\nNCseXVPcd/2v1/Jg005++fLuw5Zb68WLyGhELtjbezMkE8aUisFXUCqlGOxTvGDPOzfoDNWmbYcG\nPe7QgGmQUyuTNNRVsbu1l0de3lXcvr+jl/0d3gDqHZ86i+qKBLc9tZlLf/Q8v1jtHbf9YHfx+DW7\n2wDYcaibUlZvO8jir/w/XtxycER1FBEpiFywd/RmqKtOYWYUYj24ouNQ6vyW+jQ/2J2j2GI/EjWV\nKRprq4r959/+xOl89PTj2N+RZn+712KfU19d7OZ5bWdbMfCDJ0rt91v36/d2UMof1u4H4OmN4b8k\nOtNZfvLMFrrSWTY1d3LZj55Xl46IhEQu2Pe3p4uDoIXlA85ZNOuwjyl0wdRP8VruNZXJ0GJhQe85\nsfRzTa1K0lBbWbx/9gkzmV1Xxf72NPv8KY9z6qtCg66F9dx3HurGOUdfNl+cOrluT/uQM3l2tfbw\nQ//i2wO7Y3745Cau//Varn3kdW5/ZgvPbjrA13+1dtBzDKejN3PYNW9au/uKdRKRaIlcsG870M3C\nWVMBmFqZ4uHPvodbrzjzsI8Z2MdeU5UqXhs1qLoiwX1XnsOWmy7miWvOK24vjNHWVqVY2FBT3D5v\nWjWz66royeR4c08H1RUJZkytDK0/s9mfadObydPcmabZnyp5zqKZdPXlWL+3g2/+9k0+fMvT5P1F\nyf7+B88Vr9NamHFT8PCfva6dJ97cz5Prvdb842/uY/8RhPD3ntjIaSse40dPby5uu/n367jvhe3F\n+x/53jOc/c0nitM209kcHb0Z7nthO1mtYy8yqZUl2M1sppk9YmZdZrbNzC4rx/MOlM87th3sYuGs\n/nB91/wZ1A6YDXPKvPrQ/YF97LXVqUGPAVj3jYsAMDPm1lcXt8+u8/5CWDBraui5U8kEs+u9fSs3\n7Gfp3HoSCeNzf3NS6HmXzKkD4M09HcVW8MWnzQPgq//1Brc9tZk1u9v5+qNr2HGoOzRVcuX6Zn78\n9Gacc7R1Z9jV2sPSuXV0prPsau3hsrPnk807Hlq9s+T7tnrbIe5/cXtx5s6T/kDx/S/uIJPL09KZ\n5vt/2sS1j7xOS2ea/e29xdlCKzfsp6UzzRnX/4HTVjzGtY+8zh/W7iv5WiIy8crVYv8+0AfMAS4H\nbjWzZWV67qL9Hd7c8gWBVvNQfv25c3nrxouK9wst9UIbva4qxdumTynur6lMhoIcCE2dLAy0LpxV\nE/pSATh13jQS5s2XP/U4L/SvuXAJW266mJTfVXTBqbNJGDyzsZmHmrwAftf8GXz09OPYsK+Ddxw/\nDYC7ntvGeTevBOCX//Je/u1DS0hn89zwmzc5/euPcdXdTQBcfvb84uv/t9OP45xFM/nZS9vJ5x1d\n6Syfu/9lrvjxC+xq7SGfd/zLvX/m3x9+nWseepVc3vHmnnYaaivZ0tLFJ37wHD99dmvx+f77nS/x\nge88Vbz/6Tub+Nj3V9Hdl6PKH5f49mPruevZraEpnOOp0IVVsL+9l9++vmfIbqyhHluwtaWLf33g\nFZ7ffIC2Hl2gXOJrzBfaMLMa4BLg7c65TuAZM/sV8A/Al8b6/EGFk4kKXTGleH3vxjv/Yjqv7Gil\nyj+R6O1vm8Z7T5rFtRefgpnxl4tm8dzmA7zytQtH9PonNNSQTBj/47wTObHRC/glc+u45dIz+PwD\nr3LuSQ3FY82MRY01bNjXycmz6zhlXj0/enoLAKfOq2dRYw3fu/SM4vEHu/p41zf+ULy/ZE4d1RUJ\n/s/vvDNl23uzvODPkLng1Dn8x5/eYl97mmXH1XPpu+fzP3/2Couu/S0nNtYUT7T6u1uf5bS3TWNv\ney8nz67lN6/t4Tev7QHgax9dRm11is8/+Aqv7GjljPnTOXP+DH78zJZiGV788vu56qereWVHKw21\nlbz05Qv47hMb+b+Pb+Rrv1rDT1Zt4R/OWcCy46Yxq7aS1u4MubxjV2sPr+9spSeT47Tjp1NbleRD\ny+YxpTLphXTOG2d4cctB3nH8dFIJY8O+DqpSSRbPrWXG1EraezL8bs1eHl+7j1d2tNLak2HxbO8v\nlb3+8g/vPmEmH1o2l8Vz6phSmWD61Eq2NHfx6s5WDnT1sae1h1WbDjC7roqqVIJ97Wk609nijKYP\nnzaPf/3AYhbMmjpollSBcw6zEc6rHULefz9m1FSG/kps780wtSJJyn/dfN7hYNCy02ORzuboy+ap\nq66gK51lT1svz20+wFkLZ7B0bv3wT3AUOOfozeSZUpmc6KLEio31ohFmdgbwrHNuSmDbF4DznHMf\nHeoxy5cvd01NTaN6va50llTSqEoN/x/hQGeaHzy5iS98cMmQx6ezOdp7sjTWVQ3xaLj9mS001Fby\nyo5W7li1lZe+fEHJYwsX2A56Y1cbm5o7+fBp89i4v5PfvbGXTC7PNRcuGfID/L8eepWHVu/k3JMa\nuOczZ+Oc454XtvPsWy2856QGTplbx8r1zVxz4WLaejKs39vB2Ytmkc7mOP/mlexp62VRQw3XXnwK\njXVV/PvDr7P9YDfvWjCDH15xJve+sI21/oDtdR85lYbaKla91cJ9L27nyvct4p1/MZ2DXX38450v\n8VcnN3DNhUvYdqCLrz+6livft4i/PHEWvZkc//HHtzh+xhRueWIju9uG7tuvSiVw9A/+ViS9q1v1\nZPqvYjUSJzTU8O6FM5lRU8krOw4xu66aGVMrmFqV4herdxZnGAUlzOsmm1NfxXmLG2ntztDanaG6\nIsln//pEVq7bz87WnuJ4RTJhzJhaSTJB8aS33kyOhBmHuvuYXVcdOmciWPyBHx9HeENPX452f0ZU\nY11V8a+Mg119JAxm1lQBjvaeLH25fLHslckEqaR34ZgK/5KPCTOy+Tz5fP97jHlfCjnnyGQdmVye\nvlyeTC5fHMSfWVNJa3df6H2vq05RlfJep60nQ845pk2pYEpFkpxz5PNe6CYSRsKMZKJ/Flrwfcg7\n5/34x+edt80Rvp/PO1zhduEYBznnyOUd9dUppk2toLkjTSqRoKJQ92SiOOutN5Mnnc2R9PcbXgMq\n4f+7JWxwGUsq3/fnEfvrJbO57iOnjuqxZrbaObd82OPKEOzvAx5yzs0NbLsSuNw5d35g21XAVQDz\n588/c9u2bWN63aMpk8uz61BPaOB0sulMZ5lSkSxri2842Vye1dsO0bTtEDNrKplbX83G/R2cOm8a\nyxfOIJnwgrFp6yFWvdVCdUWSKRVJplQmqa7wur860xnyDk6aXUtbd4Y9bT0c6OqjMpXgvMWNnDqv\nvmSL2TlHS2cfG/Z10JvJsbe9lxNm1XDKvHqmT60YtqW9cV8Hr+9qY3NzFwe6+nB+yDgoBlxddYoD\nnX2DciD41APjJLTPjFOPq6e9J8Pm5i6qKhI451gwq4audJbmjjSJhPkrkKbI5PJk8nkyWUc2nyeT\n88I6m8uTc97lH5MJwzmvYeKApB+8lckEFSkvECtTCWorU6SSCbYf7GJWTRVzp1Vzyrx6Vr3VwsGu\nPu8LIOu1lqtSCdp7svRkciT9MDfrD+WBK2I75/wwxT/Wu20D7ieK263/+IQVj0uYNwliX3svbT0Z\nGmurcHifub6s9yWVy3tfBlMqElSlkmTzjlw+T955X6zO/3IZYtXukv9vysExuu+Hd82fwafPPWFU\nr3k0g/0MYJVzbmpg2zXA+ePRYhcROVaNNNjLMXi6AUiZ2cmBbacDa0ocLyIi42jMwe6c6wIeBq43\nsxozey/wt8DdY31uERE5cuWa7vhZYAqwH7gf+GfnnFrsIiITYMzTHQGccweBj5XjuUREZGwit6SA\niIgcnoJdRCRmFOwiIjGjYBcRiZkxn6A0qhc1awbGcuppA9BSpuJMdsdSXeHYqu+xVFc4tuo7XnVd\n4JxrHO6gCQn2sTKzppGcfRUHx1Jd4diq77FUVzi26jvRdVVXjIhIzCjYRURiJqrBfttEF+AoOpbq\nCsdWfY+lusKxVd8JrWsk+9hFRKS0qLbYRUSkBAW7iEjMRCrYzWymmT1iZl1mts3MLpvoMo2WmV1t\nZk1mljazOwfse7+ZrTOzbjP7k5ktCOyrMrOfmFm7me01s88f9cIfIb/Mt/v/Zh1m9rKZXRTYH6v6\nApjZPWa2xy/3BjP7TGBf7OoLYGYnm1mvmd0T2HaZ/+/eZWa/NLOZgX2R/Dyb2Uq/np3+z/rAvslR\nX+dcZH7wlgR+AKgFzgXagGUTXa5R1uXjeCti3grcGdje4NfrE0A1cDPwfGD/TcDTwAzgFGAv8KGJ\nrs8wda0BVgAL8RoTHwE6/Puxq69f7mVAlX97qV/uM+NaX7/sj/llvyfwHnQAf+V/Zu8DfhY4PpKf\nZ2Al8JkS/+aTor4T/iYdwZtZA/QBiwPb7ga+NdFlG2O9bhgQ7FfhXRw8WO8eYKl/fxdwYWD/N4L/\neaLyA7wGXHIs1BdYAuwB/j6u9QU+CTyI9wVeCPZvAvcFjjnR/wzXRfnzfJhgnzT1jVJXzGIg55zb\nENj2Kt5Z72ZjAAACi0lEQVS3ZJwsw6sXULxC1SZgmZnNAI4L7ieC74GZzcH791xDjOtrZv9pZt3A\nOrxg/y0xrK+Z1QPXA9cM2DWwrpvww43of55vMrMWM1tlZuf72yZNfaMU7LV4f7oEteF9G8bJ4epZ\nG7g/cF8kmFkFcC9wl3NuHTGur3Pus3hlfR/e5SPTxLO+3wBud87tGLB9uLpG9fP8RWAR8Da8+eqP\nmtmJTKL6RinYO4H6Advq8fq04uRw9ewM3B+4b9IzswTen599wNX+5tjWF8A5l3POPQMcD/wzMauv\nmb0TuAD4zhC7h6trJD/PzrkXnHMdzrm0c+4uYBVwMZOovlEK9g1AysxODmw7He/P+ThZg1cvAMys\nBq+vbo1z7hDen/SnB46PxHtgZgbcDswBLnHOZfxdsazvEFL49SJe9T0fbxB8u5ntBb4AXGJmf2Zw\nXRcBVXif5Th9nh1gTKb6TvRAxBEOWvwMb2S5BngvERlFL1GXFN6siJvwWrHV/rZGv16X+Nv+N+FZ\nE98CnsSbNbEULwgm/awJ4AfA80DtgO2xqy8wG28wsRZIAh8EuoC/jVt9ganA3MDPt4Gf+/VcBrTj\ndUXVAPcQniUSuc8zMN3/9yx8Xi/3/22XTKb6TvgbdYRv6kzgl/4buR24bKLLNIa6rMD7pg/+rPD3\nXYA34NaDNwK/MPC4KuAn/n+gfcDnJ7ouI6jrAr9+vXh/khZ+Lo9pfRv9cG71y/06cGVgf6zqO6Du\nK/Bnxfj3L/M/q13AfwEzA/si93n2/21fwutCacVrrHxgstVXa8WIiMRMlPrYRURkBBTsIiIxo2AX\nEYkZBbuISMwo2EVEYkbBLiISMwp2EZGYUbCLiMSMgl1EJGb+P2lbXFsaGtVZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17dcdfa3fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "npArrayDF = np.array(df_new.iloc[:,:], dtype=np.float) #shuffle randomly all samples\n",
    "\n",
    "print(npArrayDF[0,-1])\n",
    "\n",
    "for i in range(len(npArrayDF)):\n",
    "    npArrayDF[i,-1] = (npArrayDF[i, -1]) - 1\n",
    "\n",
    "print(npArrayDF[0,-1])\n",
    "    \n",
    "np.random.shuffle(npArrayDF)\n",
    "\n",
    "X_train = np.array(npArrayDF[:-2000,:-1], dtype=np.float)\n",
    "y_train = np.array(npArrayDF[:-2000,-1], dtype=np.float)\n",
    "\n",
    "X_valid = np.array(npArrayDF[-2000:-1000,:-1], dtype=np.float)\n",
    "y_valid = np.array(npArrayDF[-2000:-1000,-1], dtype=np.float)\n",
    "\n",
    "X_test = np.array(npArrayDF[-1000:,:-1], dtype=np.float)\n",
    "y_test = np.array(npArrayDF[-1000:,-1], dtype=np.float)\n",
    "print(y_test[1])\n",
    "\n",
    "plt.plot(X_train[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "\n",
    "        dnn_outputs = self._dnn(X)\n",
    "\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                                  logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Make the important operations available easily through instance variables\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Translate the labels vector to a vector of sorted class indices, containing\n",
    "        # integers from 0 to n_outputs - 1.\n",
    "        # For example, if y is equal to [8, 8, 9, 5, 7, 6, 6, 6], then the sorted class\n",
    "        # labels (self.classes_) will be equal to [5, 6, 7, 8, 9], and the labels vector\n",
    "        # will be translated to [3, 3, 4, 0, 2, 1, 1, 1]\n",
    "        self.class_to_index_ = {label: index\n",
    "                                for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "\n",
    "        # needed in case of early stopping\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "\n",
    "        # extra ops for batch normalization\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        # Now train the model!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch,\n",
    "                                                                self._y: y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # If we used early stopping then rollback to the best model found\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(self.predict_proba(X), axis=1)\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        print(class_indices)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)\n",
    "\n",
    "    def restore(self, path, X, y):\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "\n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        \n",
    "        self._saver.restore(self._session, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "def fitAndPredict (name, model, trainX, trainY, validX, validY):\n",
    "\tk = 3\n",
    "\t#kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tscores = cross_val_score(model, trainX, trainY, cv = k, n_jobs = 4) #Evaluate a score by cross-validation\n",
    "\thitRate = np.mean(scores) #calculates the average\n",
    "\n",
    "\tmsg = \"{0}'s hit rate: {1}\".format(name, hitRate)\n",
    "\tprint (msg)\n",
    "\treturn hitRate, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8ef151e45601>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                     optimizer_class=tf.train.AdagradOptimizer)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhitRate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturnMessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfitAndPredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DNN tensorflow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-d6dd799576b6>\u001b[0m in \u001b[0;36mfitAndPredict\u001b[1;34m(name, model, trainX, trainY, validX, validY)\u001b[0m\n\u001b[0;32m      2\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;31m#kfold = model_selection.KFold(n_splits=10, random_state=seed)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Evaluate a score by cross-validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mhitRate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calculates the average\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "dnn = DNNClassifier(batch_size=100, learning_rate=0.05, \n",
    "                    n_hidden_layers=3, n_neurons=700, \n",
    "                    optimizer_class=tf.train.AdagradOptimizer)\n",
    "\n",
    "hitRate, returnMessage = fitAndPredict('DNN tensorflow', dnn, X_train, y_train, X_valid, y_valid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
